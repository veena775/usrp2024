/var/spool/slurmd/job2264423/slurm_script: line 13: conda: command not found
[I 2024-10-24 02:29:56,251] A new study created in RDB with name: MPk_nwLH_params_200
[W 2024-10-24 02:29:56,542] Trial 0 failed with parameters: {'h1': 20, 'dr': 0.13611865710362792} because of the following error: RuntimeError('Error(s) in loading state_dict for model_1hl:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([15, 79]) from checkpoint, the shape in current model is torch.Size([20, 79]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([15]) from checkpoint, the shape in current model is torch.Size([20]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([7, 15]) from checkpoint, the shape in current model is torch.Size([7, 20]).').
Traceback (most recent call last):
  File "/home/vk9342/.conda/envs/usrp/lib/python3.11/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/vk9342/USRP2024/pytorch/main_continue.py", line 53, in __call__
    model.load_state_dict(torch.load(fmodel))
  File "/home/vk9342/.conda/envs/usrp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2189, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for model_1hl:
	size mismatch for fc1.weight: copying a param with shape torch.Size([15, 79]) from checkpoint, the shape in current model is torch.Size([20, 79]).
	size mismatch for fc1.bias: copying a param with shape torch.Size([15]) from checkpoint, the shape in current model is torch.Size([20]).
	size mismatch for fc2.weight: copying a param with shape torch.Size([7, 15]) from checkpoint, the shape in current model is torch.Size([7, 20]).
[W 2024-10-24 02:29:56,543] Trial 0 failed with value None.
CUDA Available
Traceback (most recent call last):
  File "/home/vk9342/USRP2024/pytorch/main_continue.py", line 150, in <module>
    study.optimize(objective, n_trials, n_jobs=n_jobs)
  File "/home/vk9342/.conda/envs/usrp/lib/python3.11/site-packages/optuna/study/study.py", line 451, in optimize
    _optimize(
  File "/home/vk9342/.conda/envs/usrp/lib/python3.11/site-packages/optuna/study/_optimize.py", line 62, in _optimize
    _optimize_sequential(
  File "/home/vk9342/.conda/envs/usrp/lib/python3.11/site-packages/optuna/study/_optimize.py", line 159, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vk9342/.conda/envs/usrp/lib/python3.11/site-packages/optuna/study/_optimize.py", line 247, in _run_trial
    raise func_err
  File "/home/vk9342/.conda/envs/usrp/lib/python3.11/site-packages/optuna/study/_optimize.py", line 196, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "/home/vk9342/USRP2024/pytorch/main_continue.py", line 53, in __call__
    model.load_state_dict(torch.load(fmodel))
  File "/home/vk9342/.conda/envs/usrp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2189, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for model_1hl:
	size mismatch for fc1.weight: copying a param with shape torch.Size([15, 79]) from checkpoint, the shape in current model is torch.Size([20, 79]).
	size mismatch for fc1.bias: copying a param with shape torch.Size([15]) from checkpoint, the shape in current model is torch.Size([20]).
	size mismatch for fc2.weight: copying a param with shape torch.Size([7, 15]) from checkpoint, the shape in current model is torch.Size([7, 20]).
