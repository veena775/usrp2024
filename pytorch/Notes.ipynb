{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1518c69-fcb0-4ab1-a1ed-08c951a29ea3",
   "metadata": {},
   "source": [
    "## October 24th \n",
    "\n",
    "Early stopping\n",
    "optuna.terminator\n",
    "once youre reasomably certain that I have a hyper parameter  just fix it at that \n",
    "\n",
    "figure out what is the right amount of epoch and say max num epochs 1000 epochs and then just stop it when it hasnâ€™t imporoved and tell it to stop there\n",
    "\n",
    "implicit prior - having a dataset but weaer statement so some points go over one \n",
    "not fantastic amount of data \n",
    "\n",
    "replacing last layer with new layer - \n",
    "\n",
    "do one big heryper parameter optimzation run and then just use that \n",
    "\n",
    "optuna run - add hidden layer, suggest range between 1 and 5 for hidden layers \n",
    "\n",
    "try to see if we can get Gaussian negative likely hood \n",
    "\n",
    "should be predicting log sigma (more stable, not negative) and feed 10^logsigma to the loss function; MSE no formal sense of uncertainty; \n",
    "\n",
    "see if I can switch out ReLu\n",
    "pick at activation function and forces output to be between 0 and 1; sigmoied activation function\n",
    "\n",
    "\n",
    "defined relu but never use it \n",
    "\n",
    "code sent to slack:\n",
    "my code a bit redundant that I can only change manually \n",
    "\n",
    "1. Dont run optuna each time, run it until I am decently certain in the hyper parameters and then fix those for the rest of the runs so that it doesnt take as much time. **1 DONE (optuna pruning)**\n",
    "2. Instead of using one fixed hidden layer, make it dynamic and set it to try between 1 and 5  **maybe, see later**\n",
    "3. try to see if we can get Gaussian negative likelyhood **what**\n",
    "4. Should be predicting log sigma (more stable, not negative) and feed 10^logsigma to the loss function;  **2 NOT working**\n",
    "5. Some of the output values on the plot arent actually between 0 and 1 so try to pick at activation function such as sigmoid activation function and force output to be between 0 and 1   **2 NOT Working**\n",
    "6. Take a look at the code Christian sent since my current code a bit redundant that I can only stuff change manually   **3 later**\n",
    "7. Dynamic number of epochs   **1 DONE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ef6be-710e-4dee-8355-49ac2d11d9d0",
   "metadata": {},
   "source": [
    "# Nov 7th (just adrian)\n",
    "\n",
    "P (LCDM) $\\rightarrow$ MSE $\\rightarrow$ S (summary 5d) (alr have this, just train on LH) \n",
    "\n",
    "Ive trained my network and we have validation and test set, after this just use test set\n",
    "MSE is trained\n",
    "Take trained thing and take \n",
    "\n",
    "P ($\\nu$) $\\rightarrow$ trained MSE $\\rightarrow$ $S_{\\nu}$ (5d) here just evaluating\n",
    "\n",
    "$S_{\\nu}$  $\\rightarrow$ MSE $\\rightarrow$ 7d\n",
    "\n",
    "\n",
    "we have a lot of LCDM so can afford to use a 30+ thingg ie Pk - get 5 param sumerl neutrino case not as many sims - training w 5 dim summary instead of 30dim Pk shoukdk help the training\n",
    "\n",
    "make qa script that takes a power spectrum and evaluate sthe moddel and saves the summary to a file\n",
    "\n",
    "could write three sepparate files or try to make it the same thing, but give it an option to either read int the Pk or summary \n",
    "\n",
    "$P_{\\lambda}$ learn $N_{\\lambda}$\n",
    "$P_{\\nu} \\rightarrow N_{\\lambda}$ learn $N_{\\nu}$\n",
    "\n",
    "either add an extra layer to the lambda network too get the nu network or change the lambda netwrok to neutrino\n",
    "start w kambda wights and then let them change to get neutrino wights\n",
    "\n",
    "\n",
    "let multiple layer functionality: use dynamic\n",
    "\n",
    "wnat to show that doing this trains faster than doing neutrinos directly \n",
    "\n",
    "\n",
    "compare constraints from diff cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cef859-5c90-41c6-8ac1-370b2c7551c9",
   "metadata": {},
   "source": [
    "## Nov 14th\n",
    "\n",
    "R^2 - is this generally a line\n",
    "degeneracy between sigma 8 and neutrino mas, prior on sigma 8 is huge relative to posterior, but for neutrino mass priro is basically posterior?\n",
    "\n",
    "laytent spaces - can be whatever they want \n",
    "\n",
    "make it so that the final hidden layer, is fixed number of units eg 3 layers: l1 optuna, l2 optuna, l3 fixed, and the out 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1adfb5-f987-4fb3-9427-83ce0d58670d",
   "metadata": {},
   "source": [
    "## For Nov 26th\n",
    "Currently: <br>\n",
    "main_fixed_final.py <br>\n",
    "dynamic_model_fixed in architecture.py <br>\n",
    "test_transfer.ipynb <br>\n",
    "Have code that: <br>\n",
    "saves final hidden layer output <br>\n",
    "units of final hidden layer are input params <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376775f-e4e6-4f77-a8a0-5618f01d324a",
   "metadata": {},
   "source": [
    "sub function before final layer <br>\n",
    "make one function that does up to final layer and then all function before final layer <br>\n",
    "in main <br>]\n",
    "saving nn waights, use nn1 weights, forget weights in final layer <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a9d71-7af3-42ae-8469-67b6bcc71c64",
   "metadata": {},
   "source": [
    "# Dec 5th\n",
    "Trained just LH, dynamic model, final hidden layer output 497  <br>\n",
    "passed nwLH Pks through said model:  <br>\n",
    "Pk_nwLH: <br>\n",
    "self.mother = '/scratch/network/vk9342/USRP2024/pytorch/Pk_nwLH/transfer_497/'   <br>\n",
    "study_name       = 'Pk_nwLH_params_transfer_497'   <br>\n",
    "model = architecture.dynamic_model_fixed_final\n",
    "\n",
    "training LH model with fixed final hidden layer 5:\n",
    "self.mother = '/scratch/network/vk9342/USRP2024/pytorch/Pk_LH/fixed_final_5/'\n",
    "model = architecture.dynamic_model_fixed_final <br>\n",
    "study_name       = 'Pk_LH_params_fixed_final_5'   #+str(epochs) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ffeca0-cdfc-4785-8483-ebd851f84674",
   "metadata": {},
   "source": [
    "try with new seed - LH (now nwLCDM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141c585-4b4b-4f5e-a73a-bd83fe1f728f",
   "metadata": {},
   "source": [
    "## Dec 12th\n",
    "Free final layer in second network <br>\n",
    "Side by side table of R^2 for lcdm, nwLH without transfer learning, nwLH with transfer learning <br>\n",
    "Same thing but for MPk <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a4a35-3704-4387-acfa-4e219cc57001",
   "metadata": {},
   "source": [
    "## JAN\n",
    "\n",
    "Pk\n",
    "train network on LH with 5 fixed final layers:\n",
    "- input Pk 'all_Pk_LH.npy'; \n",
    "- input params: '../real_params/latin_hypercube_params.txt'\n",
    "- mother: '/scratch/network/vk9342/USRP2024_scratch/pytorch/Pk_LH/transfer5_network1/\n",
    "- study name:  Pk_LH_params_transfer5_network1\n",
    "- reuslts name:  ..._Pk_LH_params_transfer5_network1\n",
    "- model: dynamic_model_fixed_final\n",
    "- make sure Pk is log in data\n",
    "\n",
    "pass nwLH params through final layer:\n",
    "- input Pk: 'all_Pk_nwLH.npy'\n",
    "- input params: '../real_params/latin_hypercube_params_nwLH.txt'\n",
    "- results name: Pk_nwLH_params_transfer5\n",
    "- all save new Pks and params as npy and txt files:\n",
    "- 'all_Pk_nwLH_fortransfer.npy';\n",
    "- in data dont log Pk\n",
    "- dont shuffle in data.py\n",
    "- '../real_params/latin_hypercube_params_nwLH.txt'\n",
    "\n",
    "train new network on these reduced nwLH values; no fxed final layer \n",
    "- input Pk: 'all_Pk_nwLH_fortransfer.npy'\n",
    "- input params: '../real_params/latin_hypercube_params_nwLH.txt'\n",
    "- mother: '/scratch/network/vk9342/USRP2024_scratch/pytorch/Pk_nwLH/transfer5_network2/\n",
    "- study name: Pk_nwLH_params_transfer5_network2  \n",
    "- reuslts name:  ..._Pk_nwLH_params_transfer5_network2\n",
    "- in data dont log Pk\n",
    "- v2 is w 'dynamic' architecture\n",
    "\n",
    "train 1 network on only nwLH:\n",
    "- input Pk: 'all_Pk_nwLH.npy;\n",
    "- input params: '../real_params/latin_hypercube_params_nwLH.txt'\n",
    "- mother: '/scratch/network/vk9342/USRP2024_scratch/pytorch/Pk_nwLH/no_transfer/' \n",
    "- study name: Pk_nwLH_params\n",
    "- results name: _Pk_nwLH_params\n",
    "- make sure Pk is log in data\n",
    "\n",
    "**MPk:**\n",
    "all same but with MPk\n",
    "\n",
    "train 1 network on only nwLH:\n",
    "- input MPk: 'all_MPk_nwLH.npy;\n",
    "- input params: '../real_params/latin_hypercube_params_nwLH.txt'\n",
    "- mother: '/scratch/network/vk9342/USRP2024_scratch/pytorch/MPk_nwLH/no_transfer/' \n",
    "- study name: MPk_nwLH_params_ *typo so extra underscore\n",
    "- results name: _MPk_nwLH_params\n",
    "- make sure MPk is log in data\n",
    "\n",
    "train network on LH with 5 fixed final layers:\n",
    "- input MPk 'all_MPk_LH.npy'; \n",
    "- input params: '../real_params/latin_hypercube_params.txt'\n",
    "- mother: '/scratch/network/vk9342/USRP2024_scratch/pytorch/MPk_LH/transfer5_network1/\n",
    "- study name:  MPk_LH_params_transfer5_network1\n",
    "- reuslts name:  ..._MPk_LH_params_transfer5_network1\n",
    "- model: dynamic_model_fixed_final\n",
    "- make sure MPk is log in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe5a735-4f66-4349-a716-ef244c037170",
   "metadata": {},
   "source": [
    "## FEB\n",
    "test MPk:\n",
    "main_original.py\n",
    "- self.mother = '/scratch/network/vk9342/USRP2024_scratch/pytorch/MPk_nwLH/test_MPk/'\n",
    "- f_Pk = 'all_MPk_nwLH.npy'\n",
    "- f_params  = '../real_params/latin_hypercube_params_nwLH.txt'\n",
    "- study_name       = 'MPk_nwLH_params_test_1hl_300'\n",
    "- epochs     = 300\n",
    "- model = architecture.model_1hl(self.input_size, h1, self.output_size, \n",
    "                            dr).to(self.device)\n",
    "\n",
    "results mid af\n",
    "\n",
    "test 2:\n",
    "main.py\n",
    "- self.mother = '/scratch/network/vk9342/USRP2024_scratch/pytorch/MPk_nwLH/test_MPk/'\n",
    "- f_Pk = 'all_MPk_nwLH.npy'\n",
    "- f_params  = '../real_params/latin_hypercube_params_nwLH.txt'\n",
    "- study_name       = 'MPk_nwLH_params_test_1hl_dynamic_epochs'\n",
    "- epochs     = 1000\n",
    "- model = architecture.model_1hl_sigmoid(self.input_size, h1, self.output_size, \n",
    "                            dr).to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac194a-0043-4c3c-8530-ad40a5d396c1",
   "metadata": {},
   "source": [
    "### terminal comands:\n",
    "cd ../../scratch/network/vk9342 <br>\n",
    "cd USRP2024  <br>\n",
    "cd pytorch/  <br>\n",
    "conda activate usrp  <br>\n",
    "sbatch pytorch_GPU.job <br>\n",
    "tail -f /home/vk9342/USRP2024/pytorch/joboutput/mse.o2321223  change number at the end  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800318a1-996f-4747-bd09-a442339f77c3",
   "metadata": {},
   "source": [
    "### BSQ:\n",
    "\n",
    "Network 1:\n",
    "- cosm_type = 'BSQ'\n",
    "- Pk_type = 'Pk' or 'MPk'\n",
    "- name = 'transfer5_network1'\n",
    "- f_params  = '../real_params/BSQ_params.txt'\n",
    "- f_Pk      = 'all_'+str(Pk_type)+'_'+str(cosm_type)+'.npy'\n",
    "- study_name  = str(Pk_type)+'_'+str(cosm_type)+'_params_'+str(name)   #+str(epochs)\n",
    "\n",
    "Network 2:\n",
    "- cosm_type = 'nwLH'\n",
    "- Pk_type = 'Pk' or 'MPk'\n",
    "- name = 'transfer5_network2_BSQ'\n",
    "- f_params  = '../real_params/latin_hypercube_params_'+str(cosm_type)+'.txt' \n",
    "- f_Pk      = 'all_'+str(Pk_type)+'_BSQ_fortransfer.npy'\n",
    "- study_name  = str(Pk_type)+'_'+str(cosm_type)+'_params_'+str(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd75696c-db30-4270-8a13-13d3d757ce00",
   "metadata": {},
   "source": [
    "## Feb 13th\n",
    "\n",
    "Get rid of w <br>\n",
    "\n",
    "Final plot, <br>\n",
    "\n",
    "sigma or r^2 (pref sigma ie error bar of each param) as a function of N sim train,  <br>\n",
    "tranfer learning should be below  <br>\n",
    "\n",
    "One explanation is that hasnt converged yet, 2000 sims isnt enough <br>\n",
    "\n",
    "try again with log\n",
    "\n",
    "use in data.py w only one range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c8b22-96f1-40b5-9700-19506a7f31f2",
   "metadata": {},
   "source": [
    "### Feb 15th: testing:\n",
    "cosm_type = 'nwLH' <br>\n",
    "Pk_type = 'MPk'    <br>\n",
    "name = 'no_transfer_w_log'   <br>\n",
    "log = True   <br>\n",
    "study_name  = str(Pk_type)+'_'+str(cosm_type)+'_params_'+str(name)   #+str(epochs)  <br>\n",
    "\n",
    "f_Pk      = 'all_'+str(Pk_type)+'_'+str(cosm_type)+'.npy'  <br>\n",
    "f_params  = '../real_params/latin_hypercube_params_'+str(cosm_type)+'.txt'   <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa76ffb-426e-4155-8260-8390bd4cb3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
