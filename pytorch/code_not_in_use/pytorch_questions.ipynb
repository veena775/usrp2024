{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68223f-2c59-4bb8-b0f3-f4e92590d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f5ca65-30b0-4df4-9897-a0ca32d9e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## GOOGLE COLAB #####################################\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "################## Pylians3 example #####################################\n",
    "# use GPUs if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "\n",
    "################## GOOGLE COLAB #####################################\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "################## Pylians3 example #####################################\n",
    "######## 2 hidden layers #########\n",
    "# inp ---------> size of input data\n",
    "# h1 ----------> size of first hidden layer\n",
    "# h2 ----------> size of second hidden layer\n",
    "# out ---------> size of output data\n",
    "# dr ----------> dropout rate\n",
    "class model_2hl(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp, h1, h2, out, dr):\n",
    "        super(model_2hl, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(inp, h1) \n",
    "        self.fc2 = nn.Linear(h1,  h2)\n",
    "        self.fc3 = nn.Linear(h2,  out)\n",
    "\t\n",
    "        self.dropout   = nn.Dropout(p=dr)\n",
    "        self.ReLU      = nn.ReLU()\n",
    "        self.LeakyReLU = nn.LeakyReLU()\n",
    "        \n",
    "        # initialize the weights of the different layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 1)\n",
    "            elif isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose2d) or \\\n",
    "                 isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "          \n",
    "################## GOOGLE COLAB #####################################\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "################## Pylians3 example #####################################    \n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        out = self.dropout(self.LeakyReLU(self.fc1(x)))\n",
    "        out = self.dropout(self.LeakyReLU(self.fc2(out)))\n",
    "        out = self.fc3(out)         \n",
    "        return out\n",
    "\n",
    "\n",
    "################## GOOGLE COLAB #####################################\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")\n",
    "\n",
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())\n",
    "\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")\n",
    "\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "\n",
    "\n",
    "\n",
    "################## Pylians3 example ##################################### \n",
    "\n",
    "#Dynamic model - go ti github\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d406d9-2e4f-420a-9ffa-feae0cf06966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92484201-5913-470a-896d-61c5f843b5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
