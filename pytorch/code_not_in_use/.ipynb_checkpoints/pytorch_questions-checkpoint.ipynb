{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68223f-2c59-4bb8-b0f3-f4e92590d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7a370d-b91b-48fe-92d5-393963fda805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()  ##\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)   #I don't need this right??\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9993b-68d4-46e6-8209-31a31f571051",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## 2 hidden layers #########\n",
    "# inp ---------> size of input data\n",
    "# h1 ----------> size of first hidden layer\n",
    "# h2 ----------> size of second hidden layer\n",
    "# out ---------> size of output data\n",
    "# dr ----------> dropout rate\n",
    "class model_2hl(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp, h1, h2, out, dr):\n",
    "        super(model_2hl, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(inp, h1) \n",
    "        self.fc2 = nn.Linear(h1,  h2)\n",
    "        self.fc3 = nn.Linear(h2,  out)\n",
    "\t\n",
    "        self.dropout   = nn.Dropout(p=dr)\n",
    "        self.ReLU      = nn.ReLU()\n",
    "        self.LeakyReLU = nn.LeakyReLU()\n",
    "        \n",
    "        # initialize the weights of the different layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm3d) or isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 1)\n",
    "            elif isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose2d) or \\\n",
    "                 isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "       \n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        out = self.dropout(self.LeakyReLU(self.fc1(x)))\n",
    "        out = self.dropout(self.LeakyReLU(self.fc2(out)))\n",
    "        out = self.fc3(out)         \n",
    "        return out\n",
    "##################################s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################### INPUT ##########################################\n",
    "# data parameters\n",
    "\n",
    "Pk      = '/mnt/ceph/users/camels/Software/LFI_GNN/data_preprocessing/Pk_galaxies_IllustrisTNG_LH_33_kmax=20.0.npy'\n",
    "params  = 'latin_hypercube_params.txt'\n",
    "seed      = 1           ##????????????????/\n",
    "\n",
    "# architecture parameters\n",
    "input_size         = 79\n",
    "output_size        = 5\n",
    "max_layers         = 2\n",
    "max_neurons_layers = 1000  ##????????????\n",
    "\n",
    "# training parameters\n",
    "batch_size = 32         ##????????????????/\n",
    "epochs     = 1000       ##????????????????/\n",
    "\n",
    "# optuna parameters     #????????????????????????\n",
    "study_name       = 'Pk_2_params'\n",
    "n_trials         = 1000 #set to None for infinite\n",
    "storage          = 'sqlite:///TPE.db'\n",
    "n_jobs           = 1\n",
    "n_startup_trials = 20 #random sample the space before using the sampler\n",
    "######################################################################################\n",
    "\n",
    "# use GPUs if available         #Should I do this too????\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab85fd8-eca6-4001-a180-7b9f3c806e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Load cosmological parameters and power spectrum data\n",
    "cosmo_params = np.loadtxt('latin_hypercube_params.txt')\n",
    "cosmo_params = torch.tensor(cosmo_params, dtype=torch.float32)\n",
    "\n",
    "Pk = []\n",
    "for i in range(2000):\n",
    "    k_i, Pk_i = np.loadtxt(f'/scratch/network/vk9342/latin_hypercube/{i}/Pk_m_z=0.txt', unpack=True)\n",
    "    Pk.append(Pk_i)\n",
    "\n",
    "Pk = torch.tensor(Pk, dtype=torch.float32)\n",
    "\n",
    "# Define the fully connected neural network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, ## ???? what else input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        slef.linear_relu_stack = nn.Sequential(\n",
    "              nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    ###### OR ########\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, 128)  # First layer\n",
    "        self.fc2 = nn.Linear(128, 64)          # Second layer\n",
    "        self.fc3 = nn.Linear(64, output_size)  # Output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # WHAT DO I PUT IN SELF\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define model, loss, and optimizer\n",
    "input_size = Pk.shape[1]  # Number of input features (length of P(k))\n",
    "output_size = cosmo_params.shape[1]  # Number of output features (5 cosmological parameters)\n",
    "model = PowerSpectrumNetwork(input_size=input_size, output_size=output_size)\n",
    "\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    permutation = torch.randperm(Pk.size()[0])\n",
    "\n",
    "    for i in range(0, Pk.size()[0], batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = Pk[indices], cosmo_params[indices]\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'power_spectrum_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d406d9-2e4f-420a-9ffa-feae0cf06966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92484201-5913-470a-896d-61c5f843b5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
